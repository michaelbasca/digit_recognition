{"cells":[{"cell_type":"markdown","source":["# Machine Learning Engineer Nanodegree\n## Deep Learning\n## Project: Build a Digit Recognition Program\n\nIn this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n\nIn addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n\n>**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."],"metadata":{}},{"cell_type":"markdown","source":["----\n## Step 1: Design and Test a Model Architecture\nDesign and implement a deep learning model that learns to recognize sequences of digits. Train the model using synthetic data generated by concatenating character images from [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) or [MNIST](http://yann.lecun.com/exdb/mnist/). To produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n\nThere are various aspects to consider when thinking about this problem:\n- Your model can be derived from a deep neural net or a convolutional network.\n- You could experiment sharing or not the weights between the softmax classifiers.\n- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n\nHere is an example of a [published baseline model on this problem](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf). ([video](https://www.youtube.com/watch?v=vGPI_JvLoN0))"],"metadata":{}},{"cell_type":"markdown","source":["### Implementation\nUse the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."],"metadata":{}},{"cell_type":"code","source":["from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\nimport numpy as np\nimport matplotlib.pyplot as plt"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["print(\"There are {} training examples\".format(mnist.train.num_examples))\nprint(\"There are {} validation examples\".format(mnist.validation.num_examples))\nprint(\"There are {} testing examples\".format(mnist.test.num_examples))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["key = '0123456789'\nfig, axes = plt.subplots(1, 10, figsize=(10, 2))\nfor ax, image, label in zip(axes, mnist.train.images, mnist.train.labels):\n    ax.set_axis_off()\n    ax.set_title(key[np.argmax(label)])\n    ax.imshow(image.reshape((28, 28)))\ndisplay()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["five_numbers = [0, 3, 4, 5, 7]\n\nidx_03457 = np.any(mnist.train.labels[:, five_numbers] == 1, 1) \nN = len(idx_03457)\n\nprint(\"{:.2f}% of data contain a 0, 3, 4, 5, or 7\".format(np.sum(idx_03457) / N * 100))\n\nimages_03457 = mnist.train.images[idx_03457]\nlabels_03457 = mnist.train.labels[:, five_numbers][idx_03457]\n\nprint('Shape of images with 0, 3, 4, 5, or 7: ', images_03457.shape)\nprint('Shape of labels with 0, 3, 4, 5, or 7: ', labels_03457.shape)\n\ndel mnist\n\n# space_column = np.zeros(len(labels_03457)).reshape((-1, 1))\n# labels_03457space = np.c_[labels_03457, space_column]\n\n# print('Shape of Labels with space column: ', labels_03457space.shape)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["key = '03457'\nfig, axes = plt.subplots(1, 10, figsize=(10, 2))\nfor ax, image, label in zip(axes, images_03457, labels_03457):\n    ax.set_axis_off()\n    ax.set_title(key[np.argmax(label)])\n    ax.imshow(image.reshape((28, 28)))\ndisplay()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["print(images_03457.shape)\nprint(labels_03457.shape)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["joined = np.hstack((images_03457, labels_03457))\nprint(joined.shape)\nbig_data = joined\niterations = 15\n\nfor i in range(iterations): \n  np.random.shuffle(joined)\n  big_data = np.vstack((big_data, joined))\n\nprint(big_data.shape)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["images_03457 = big_data[:, :784]\nlabels_03457 = big_data[:, 784:]\ndel big_data\nprint(images_03457.shape)\nprint(labels_03457.shape)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["i = 0\nN = labels_03457.shape[0] - 3\nnum_three_digit_img_features = 3 * images_03457.shape[1]\nnum_categories_3dig_spaces   = 3 * labels_03457.shape[1]\n\n\nthree_digits_images = np.zeros((N, num_three_digit_img_features))\nthree_digits_labels = np.zeros((N, num_categories_3dig_spaces ))\nwhile i < N:\n    num1 = images_03457[i].reshape((28, 28))\n    num2 = images_03457[i+1].reshape((28, 28))\n    num3 = images_03457[i+2].reshape((28, 28))\n    num123 = np.hstack((num1, num2, num3))\n    three_digits_images[i] = num123.ravel()\n    \n    label_1 = labels_03457[i] \n    label_2 = labels_03457[i + 1]\n    label_3 = labels_03457[i + 2]\n    three_digits_labels[i] = np.hstack((label_1, label_2, label_3)).ravel()\n    i+=1\n    \ndel images_03457\ndel labels_03457"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["three_digit_key = np.array(list('034570345703457'))\nlast_sample = three_digits_images[-1]\nlast_label = three_digits_labels[-1]\n\nprint(last_label)\nprint(''.join(three_digit_key[three_digits_labels[-1].astype(np.bool)]))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(2,2))\nax.set_axis_off()\nax.imshow(last_sample.reshape((28, 84)));\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["def remove_digit(digit, three_digit):\n    two_digit = np.copy(three_digit)\n    two_digit.reshape(28, 84)[:, (digit * 28) : ((digit + 1) * 28)] = 0\n    return two_digit.ravel()\n\ndef revise_label(digit, label):\n    label_copy = np.copy(label)\n    label_copy[digit * 5 : (digit + 1) * 5] = 0\n    return label_copy\n    \n\nremove_key = {0:'first', 1:'second', 2:'third'}\n\nfig, axes = plt.subplots(1, 3, figsize=(20, 3))\nfor digit, ax in zip(np.arange(3), axes):\n    new_digits = remove_digit(digit, last_sample)    \n    new_label = revise_label(digit, last_label)\n    \n    ax.imshow(new_digits.reshape(28, 84))\n    ax.set_title(\"{} digit removed\".format(remove_key[digit]))\n    ax.set_xlabel('Label\\n {}'.format(new_label))\ndisplay(fig)    "],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["p = 0.5 # Designate 50% of samples having a digit or two removed\n\nsample_size = int(N * p)\n\nspace_idx = np.random.RandomState(1234).choice(np.arange(N), size=sample_size, replace=False)\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["for i in space_idx:\n    num_digits_to_remove = np.random.randint(1, 3)\n    digit_places = [0, 1, 2]  \n    j = 0\n    while j < num_digits_to_remove:\n        digit_to_remove = np.random.choice(digit_places)\n        three_digits_images[i] = remove_digit(digit_to_remove, three_digits_images[i])\n        three_digits_labels[i] = revise_label(digit_to_remove, three_digits_labels[i])\n        digit_places.remove(digit_to_remove)\n        j += 1     "],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["fig, axes = plt.subplots(5, 5, figsize=(20, 10))\nfor i, ax in enumerate(axes.ravel()):\n    ax.imshow(three_digits_images[i].reshape(28, 84))\n    ax.set_title(\"{}\".format(three_digits_labels[i]), fontsize=8)\n    ax.set_axis_off()\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["print(three_digits_images.shape)\nprint(three_digits_labels.shape)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["print(three_digits_images.shape[0] * 0.75)\nprint(three_digits_images.shape[0] * 0.25)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["train_images = three_digits_images[:325000]\ntrain_labels = three_digits_labels[:325000]\n\ntest_images = three_digits_images[325000:]\ntest_labels = three_digits_labels[325000:]\n\ndel three_digits_images\ndel three_digits_labels"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["### Your code implementation goes here.\n\n# Create the model\nx = tf.placeholder(tf.float32, [None, 2352])\nW = tf.Variable(tf.zeros([2352, 15]))\nb = tf.Variable(tf.zeros([15]))\ny = tf.matmul(x, W) + b\n\n# Define loss and optimizer\ny_ = tf.placeholder(tf.float32, [None, 15])\n\n# The raw formulation of cross-entropy,\n#\n#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n#                                 reduction_indices=[1]))\n#\n# can be numerically unstable.\n#\n# So here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n# outputs of 'y', and then average across the batch.\ncross_entropy = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\n# Train\nfor i in range(5000):\n  batch_xs = train_images[(i * 65):(i + 1) * 65]\n  batch_ys = train_labels[(i * 65):(i + 1) * 65]\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n  if i%100 == 0:\n    print('step ', i)\n    \n# Test trained model\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# print(sess.run(accuracy, feed_dict={x: test_images,\n#                                     y_: test_labels}))\n\n### Feel free to use as many code cells as needed.\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":["ksflkfjlks"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["### Your code implementation goes here.\n\n####################################################################################\n#                             Helper Functions                                     #\n####################################################################################\n# Weight initialization\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\n# Convolution stride of 1 and no padding\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n# 2X2 pooling\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\n\n####################################################################################\n#                             Model Building                                       #\n####################################################################################\n\n# Create the model\n\n# First convulution layer - 1 layer 32 features\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nx_image = tf.reshape(x, [-1,28,84,1])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\n# Second convulution layer - 32 layers 64 features\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\n# Densely connected layer - 1 layer 1024 features\nW_fc1 = weight_variable([7 * 21 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*21*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n# Apply Dropout\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# Final Readout Layer\nW_fc2 = weight_variable([1024, 15])\nb_fc2 = bias_variable([15])\n\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n####################################################################################\n#                                  Model Training                                  #\n####################################################################################\n\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsess.run(tf.global_variables_initializer())\nfor i in range(5000):\n  batch_xs = train_images[(i * 65):(i + 1) * 65]\n  batch_ys = train_labels[(i * 65):(i + 1) * 65]\n  if i%10 == 0:\n    train_accuracy = accuracy.eval(feed_dict={\n        x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n  train_step.run(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.9})\n\n# print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n#     x: three_digits_images[:27081], y_: three_digits_labels[:27081], keep_prob: 1.0}))\n\n### Feel free to use as many code cells as needed."],"metadata":{"collapsed":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":["kasjlkdfldsj"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":[" print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n     x: train_images[:-1], y_: train_labels[:-1], keep_prob: 1.0}))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from six.moves.urllib.request import urlretrieve\nimport os\nimport sys\n\nlast_percent_reported = None\n\ndef download_progress_hook(count, blockSize, totalSize):\n    \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n    slow internet connections. Reports every 5% change in download progress.\n    \"\"\"\n    global last_percent_reported\n    percent = int(count * blockSize * 100 / totalSize)\n\n    if last_percent_reported != percent:\n        if percent % 5 == 0:\n            sys.stdout.write(\"%s%%\" % percent)\n            sys.stdout.flush()\n        else:\n            sys.stdout.write(\".\")\n            sys.stdout.flush()\n      \n        last_percent_reported = percent\n\n    \nurl = \"http://ufldl.stanford.edu/housenumbers/\"\n\ndef maybe_download(filename, expected_bytes, force=False):\n    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n    if force or not os.path.exists(filename):\n        print('Attempting to download:', filename) \n        filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n        print('\\nDownload Complete!')\n    statinfo = os.stat(filename)\n    if statinfo.st_size == expected_bytes:\n        print('Found and verified', filename)\n    else:\n        raise Exception(\n          'Failed to verify ' + filename + '. Can you get to it with a browser?')\n    return filename\n\ntrain_filename = maybe_download('train.tar.gz', 404141560)\ntest_filename = maybe_download('test.tar.gz', 276555967)\nextra_filename = maybe_download('extra.tar.gz', 1955489752)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["import tarfile\n\ndef maybe_extract(filename, force=False):\n    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n    if os.path.isdir(root) and not force:\n        # You may override by setting force=True.\n        print('%s already present - Skipping extraction of %s.' % (root, filename))\n    else:\n        print('Extracting data for %s. This may take a while. Please wait.' % root)\n        tar = tarfile.open(filename)\n        sys.stdout.flush()\n        tar.extractall()\n        tar.close()\n    return None\n  \nmaybe_extract(train_filename)\nmaybe_extract(test_filename)\nmaybe_extract(extra_filename)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Question 1\n_What approach did you take in coming up with a solution to this problem?_"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["### Question 2\n_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)_"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["### Question 3\n_How did you train your model? How did you generate your synthetic dataset?_ Include examples of images from the synthetic data you constructed."],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["----\n## Step 2: Train a Model on a Realistic Dataset\nOnce you have settled on a good architecture, you can train your model on real data. In particular, the [Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/) dataset is a good large-scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to perform well."],"metadata":{}},{"cell_type":"markdown","source":["### Implementation\nUse the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."],"metadata":{}},{"cell_type":"code","source":["\n\n### Your code implementation goes here.\n### Feel free to use as many code cells as needed.\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Question 4\n_Describe how you set up the training and testing data for your model. How does the model perform on a realistic dataset?_"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["### Question 5\n_What changes did you have to make, if any, to achieve \"good\" results? Were there any options you explored that made the results worse?_"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["### Question 6\n_What were your initial and final results with testing on a realistic dataset? Do you believe your model is doing a good enough job at classifying numbers correctly?_"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["----\n## Step 3: Test a Model on Newly-Captured Images\n\nTake several pictures of numbers that you find around you (at least five), and run them through your classifier on your computer to produce example results. Alternatively (optionally), you can try using OpenCV / SimpleCV / Pygame to capture live images from a webcam and run those through your classifier."],"metadata":{}},{"cell_type":"markdown","source":["### Implementation\nUse the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."],"metadata":{}},{"cell_type":"code","source":["\n\n### Your code implementation goes here.\n### Feel free to use as many code cells as needed.\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### Question 7\n_Choose five candidate images of numbers you took from around you and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?_"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["### Question 8\n_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the realistic dataset?_"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["### Optional: Question 9\n_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:** Leave blank if you did not complete this part."],"metadata":{}},{"cell_type":"markdown","source":["----\n### Step 4: Explore an Improvement for a Model\n\nThere are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Train a regression loss to the coordinates of the bounding box, and then test it."],"metadata":{}},{"cell_type":"markdown","source":["### Implementation\nUse the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."],"metadata":{}},{"cell_type":"code","source":["\n\n### Your code implementation goes here.\n### Feel free to use as many code cells as needed.\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["### Question 10\n_How well does your model localize numbers on the testing set from the realistic dataset? Do your classification results change at all with localization included?_"],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["### Question 11\n_Test the localization function on the images you captured in **Step 3**. Does the model accurately calculate a bounding box for the numbers in the images you found? If you did not use a graphical interface, you may need to investigate the bounding boxes by hand._ Provide an example of the localization created on a captured image."],"metadata":{}},{"cell_type":"markdown","source":["**Answer:**"],"metadata":{}},{"cell_type":"markdown","source":["----\n## Optional Step 5: Build an Application or Program for a Model\nTake your project one step further. If you're interested, look to build an Android application or even a more robust Python program that can interface with input images and display the classified numbers and even the bounding boxes. You can for example try to build an augmented reality app by overlaying your answer on the image like the [Word Lens](https://en.wikipedia.org/wiki/Word_Lens) app does.\n\nLoading a TensorFlow model into a camera app on Android is demonstrated in the [TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), which you can simply modify.\n\nIf you decide to explore this optional route, be sure to document your interface and implementation, along with significant results you find. You can see the additional rubric items that you could be evaluated on by [following this link](https://review.udacity.com/#!/rubrics/413/view)."],"metadata":{}},{"cell_type":"markdown","source":["### Optional Implementation\nUse the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."],"metadata":{}},{"cell_type":"code","source":["\n\n### Your optional code implementation goes here.\n### Feel free to use as many code cells as needed.\n\n"],"metadata":{"collapsed":true},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["### Documentation\nProvide additional documentation sufficient for detailing the implementation of the Android application or Python program for visualizing the classification of numbers in images. It should be clear how the program or application works. Demonstrations should be provided."],"metadata":{}},{"cell_type":"markdown","source":["_Write your documentation here._"],"metadata":{}},{"cell_type":"markdown","source":["> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2.0},"version":"2.7.13","nbconvert_exporter":"python","file_extension":".py"},"name":"digit_recognition (1)","notebookId":4159616258889972,"kernelspec":{"display_name":"Python [default]","language":"python","name":"python2"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
